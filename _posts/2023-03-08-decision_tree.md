---
layout: post
title: "机器学习 - 决策树"
subtitle: "Machine Learning"
date: 2023-03-08 14:40:00 +0800
categories: [AI]
---

# 机器学习 - 决策树(Decision Tree)

## 基本流程

- 决策树是一类常见的机器学习方法，是基于树结构来进行决策的，这恰是人类在面临决策问题时一种很自然的处理机制

- 决策过程的最终结论对应了我们所希望的判定结果，决策过程中提出的每个判定问题都是对某个属性的“测试”

- 一般一棵决策树包含一个根节点、若干个内部节点和若干个叶节点；叶节点对应于决策结果，其他每个节点则对应于一个属性测试

- 决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简单而直观的“分而治之”策略

```PseudoCode
输入: 训练集D={(x1, y1), (x2, y2), ..., (xm, ym)}
    属性集A={a1, a2, ..., ad}
过程: 函数TreeGenerate(D, A)

生成节点node;

if D中样本全属于同一类别C then
    将node标记为C类叶节点; return
end if

if A=ø OR D中样本在A上取值相同 then
    将node标记为叶节点，其类别标记为D中样本数最多的类; return
end if

从A中选择最优划分属性a*;

for a*的每一个值a*v do
    为node生成一个分支; 令Dv表示D中在a*上取值为a*v的样本子集;
    if Dv为空 then
        将分支节点标记为叶节点，其类别标记为D中样本最多的类; return
    else
        以TreeGenerate(Dv, A \ {a*})为分支节点
    end if
end for

输出: 以node为根节点的一棵决策树
```

在决策树的基本算法中，有三种情形会导致递归返回:

1. 当前节点包含的样本全属于同一类别，无需划分
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
3. 当前节点包含的样本集合为空，不能划分

> 情形2是利用当前结点的后验分布，而情形3则是把父结点的样本分布作为当前结点的先验分布

***

## 划分选择

- 由以上的伪代码中可以看出，决策树学习的关键是“从A中选择最优划分属性a*”，即如何选择最优划分属性

> 一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”越来越高

### 信息增益

- "信息熵(information entropy)"是度量样本集合纯度最常用的一种指标

假定当前样本集合D中的第k类样本所占的比例为$p_k(k=1, 2, ..., |\gamma|)$，则D的信息熵定义为:

$$
    \begin{align}
        Ent(D) = - \sum^{|\gamma|}_{k=1}p_klog_2p_k
    \end{align}
$$

- Ent(D)的值**越小**，则D的**纯度越高**

假如这个属性有n个不同的值，那么就会产生n个分支，每个分支中包含的样本数量也不同，分支中样本数量越多就说明分支结点的影响越大。

根据这个性质我们可以计算用属性a对样本集D进行划分所获得的“信息增益“:

$$
    \begin{align}
        Gain(D, a) = Ent(D) - \sum^V_{v=1}\frac{|D^v|}{|D|}Ent(D^v)
    \end{align}
$$

- 一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“**纯度提升**”越大($a_* = arg_{a \in A} \ max \ Gain(D, a)$)

> ID3决策树算法中就是以信息增益为准则来选择划分属性

### 增益率

- **信息增益准则对可取值数目较多的属性有所偏好**

为了减少这种偏好可能带来的不利影响，我们可以使用“增益率”来选择最优划分属性

> C4.5决策树算法中就是用增益率来选择最优划分属性

增益率的定义为:

$$
    \begin{align}
        Gain\_ratio(D, a) = \frac{Gain(D, a)}{IV(a)}
    \end{align}
$$

其中

$$
    \begin{align}
        IV(a) = - \sum^V_{v=1}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
    \end{align}
$$

IV称为属性a的“固有值(Intrinsic Value)”，属性a的可能取值数目越多（V越大），则IV(a)的值通常会很大

> 增益率准则对可取数目较少的属性有所偏好，因此，在C4.5算法中并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的

### 基尼指数

- CART决策树使用“基尼指数”来选择划分属性

$$
    \begin{align}
        Gini(D) & = \sum^{|\gamma|}_{k=1}\sum_{k' \neq k}p_kp_{k'} \\
                & = 1 - \sum^{|\gamma|}_{k=1}p_k^2
    \end{align}
$$

- 直观来说，Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率；Gini(D)越小，数据集D的纯度越高

属性a的基尼指数定义为:

$$
    \begin{align}
        Gini\_index(D, a) = \sum^V_{v=1}\frac{|D^v|}{|D|}Gini(D^v)
    \end{align}
$$

- 我们在候选属性集合A中，选择那个使得划分后基尼指数最小的属性作为最优划分属性

***

## 剪枝处理 Pruning

“剪枝”是决策树学习算法对付“**过拟合**”的主要手段

- 剪枝的主要思想就是如果我把当前的结点设置为叶子节点，则可以提高泛化性能，或者可以提高验证集的准度，那么就进行剪枝

### 预剪枝

> 预剪枝是指在决策树**生成**过程中，对每个结点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点

- 预剪枝基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险

### 后剪枝

> 后剪枝是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶结点

- 一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树
- 但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶节点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多
